{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 01: MR image handling, co-registration, intensity normalization\n",
    "\n",
    "The aim of this session is to get familiar with MR images, and to perform some basic pre-processing step.\n",
    "Python use is required. Exercises are indicated in red. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#System imports\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import collections\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra import\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "from joblib import Parallel, delayed\n",
    "import nibabel as nib\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get familiar with MR images\n",
    "\n",
    "The images we propose to load are in nifti format. This format embeds two kinds of information: the image itself (store in the data array), and some complementary about its acquisition (store in the header).\n",
    "The images can be seen under three views: axial, coronal or sagittal.\n",
    "Note also that the images you will see in this session and the next one show lesions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = os.path.join(os.path.dirname(os.getcwd()), \"training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\helen\\Documents\\enseignement\\epita2019\\IMED\\training\\1\\orig\\reg_T1.nii.gz\n"
     ]
    }
   ],
   "source": [
    "impath = os.path.join(datapath, \"1\", \"orig\", \"reg_T1.nii.gz\")\n",
    "print(impath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image dimension: (240, 240, 48)\n",
      "Voxel size (mm): (0.958333, 0.958333, 3.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044e701b96e347dd84fa7e2ac71768f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=24, description='mr_slice', max=48), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the MR volume\n",
    "im = nib.load(impath)\n",
    "im_arr = im.get_data()\n",
    "#print(np.unique(im_arr))\n",
    "print(\"Image dimension:\", im.shape)\n",
    "\n",
    "#Get the voxel size\n",
    "print(\"Voxel size (mm):\", im.header.get_zooms())\n",
    "sx, sy, sz = im.shape\n",
    "\n",
    "def show_axial(im_arr, mr_slice):\n",
    "    plt.imshow(im_arr[:, :, mr_slice].T, cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.pause(0.1)\n",
    "\n",
    "#Show the MR volume, slice by slice, in axial view\n",
    "interactive_plot = interactive(show_axial, im_arr=fixed(im_arr), mr_slice=(0, sz, 1))\n",
    "interactive_plot    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MR imaging offers multiple advantages:\n",
    "- no X-rays,\n",
    "- good contrast in soft tissues,\n",
    "- availability of different image types in one machine (multimodality)...\n",
    "\n",
    "For example, you are vizualizing a T1 weighted MR image, slice by slice. The darkest parts of the images represent water and the brightest fat. Here, contrast agent has been previously injected to the patient before his scan, more specifically Gadolinium, allowing to enhance the image, which will be particularly useful to better delineate tumors, as we will see later.\n",
    "Moreover, MR imaging is useful to make the distinction between the different parts of the brain. \n",
    "Let's now explore the brain !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b86576531084ea390a081765c53c65c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=24, description='mr_slice', max=48), Dropdown(description='tissue_type',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the manual segmentation of multiple tissues for the current patient\n",
    "gtpath = os.path.join(datapath, \"1\", \"segm.nii.gz\")\n",
    "gt = nib.load(gtpath)\n",
    "gt_arr = gt.get_data().astype(int)\n",
    "\n",
    "#Create the tissue list\n",
    "tissue_types = [\"Background\", \"Cortical gray matter\", \"Basal ganglia\", \"White matter\", \"White matter lesions\", \"Cerebrospinal fluid\", \n",
    "                \"Ventricles\", \"Cerebellum\", \"Brain stem\", \"Infarction\", \"Other\"]\n",
    "\n",
    "#Define the function to superimpose the MR image and regions of interest contours in axial view\n",
    "def show_manual_seg(im_arr, seg_arr, mr_slice, tissue_type, tissue_types):\n",
    "    tissue_label = np.where(np.array(tissue_types) == tissue_type)[0][0]\n",
    "    tissue_arr = (seg_arr[:, :, mr_slice] == tissue_label).astype(int)\n",
    "    plt.imshow(im_arr[:, :, mr_slice].T, cmap=\"gray\")\n",
    "    if np.count_nonzero(tissue_arr) > 0:\n",
    "        plt.contour(tissue_arr.T, colors=\"r\", linewidths=1, levels=[0.5, 1])\n",
    "    plt.axis('off')\n",
    "    plt.suptitle(tissue_types[tissue_label], fontsize=15)\n",
    "    plt.pause(0.1)\n",
    "    \n",
    "    \n",
    "#Show the MR volume, slice by slice, in axial view\n",
    "interactive_plot = interactive(show_manual_seg, im_arr=fixed(im_arr), seg_arr=fixed(gt_arr), \n",
    "                               mr_slice=(0, sz - 1, 1), tissue_type=tissue_types, tissue_types=fixed(tissue_types))\n",
    "interactive_plot    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The axial view is particularly interesting for praticians, as its allows vizualizing the relative symmetry of the brain. Moreover, a brain lesion presence may be highlighted by a region of abnormal signal compared to the symmetrical region with respect to the inter-hemispheric plane (named the contralateral).  \n",
    "Depending on the shape of the structure of interest to study, the other views may also be useful to show.\n",
    "\n",
    "<font color='red'>Show the image in sagittal view</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af82ae0b6b4c4946a5596cf4bbcc4a9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=120, description='mr_slice', max=240), Output()), _dom_classes=('widget-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_sagittal(im_arr, mr_slice):\n",
    "    plt.imshow(np.flipud(im_arr[mr_slice, :, :].T), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.pause(0.1)\n",
    "\n",
    "interactive_plot = interactive(show_sagittal, im_arr=fixed(im_arr), mr_slice=(0, sx - 1, 1))\n",
    "interactive_plot   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Show the image in coronal view.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2437f82aaec45e0a6e6ec300ad65f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=120, description='mr_slice', max=240), Output()), _dom_classes=('widget-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_coronal(im_arr, mr_slice):\n",
    "    plt.imshow(np.flipud(im_arr[:, mr_slice, :].T), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.pause(0.1)\n",
    "\n",
    "interactive_plot = interactive(show_coronal, im_arr=fixed(im_arr), mr_slice=(0, sy - 1, 1))\n",
    "interactive_plot   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voxel size is important when dealing with MR images. We are now going to prove it with two examples...\n",
    "\n",
    "<font color='red'>Compute the ventricules volume, before and after downsampling (factor 2) the manual segmentation image. Comment. What should you do to make these volumes similar ?</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Compute the distance between the two basal ganglia, before and after downsampling (factor 2) the manual segmentation image. Comment. What should you do to make these distances similar ?</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intensity normalization\n",
    "\n",
    "Normalize the intensity is a common preprocessing step in MR imaging as there is not standard scale. It is also a mandatory step to further extract texture parameters, or in supervized segmentation. Try some methods below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear image quantization with grey levels in [0, 1]\n",
    "norm_im_arr = im_arr / np.max(im_arr)\n",
    "\n",
    "#Affine image quantization on 255 grey levels\n",
    "norm_im_arr = 254 * (im_arr - np.min(im_arr)) / (np.min(im_arr) - np.max(im_arr))\n",
    "\n",
    "#Zero-mean and unit variance\n",
    "mu = np.mean(im_arr)\n",
    "std = np.std(im_arr)\n",
    "norm_im_arr = (im_arr - mu) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the MR image to have zero-mean and unit variance is commonly use in Deep learning segmentation methods (see for example the Deepmedic paper from Kamnitsask et. al, 2016). The code above normalizes the intensities on the whole image, but you can restrict the normalization to a specific tissue mask.\n",
    "\n",
    "<font color='red'>Modify the code above to have zero-mean and unit variance within the brain mask, computed by excluding the background and the cerebrospinal fluid from the manual segmentation.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> Normalize all the T1 images to have zero-mean and unit variance within the brain mask. Superimpose the obtained histograms. Are they strictly the same ? Compute a histogram matching algorithm so that the histogram of image 2 is the same as the one of image 1.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram_matching(im_arr, ref_arr, mask_arr):\n",
    "\n",
    "    #Get the intensities of the reference and the target images\n",
    "    #inside the reference mask\n",
    "    [xM, yM, zM] = np.where(mask_arr != 0)\n",
    "    im_gls = []\n",
    "    ref_gls = []\n",
    "    for ind_i, xi in enumerate(xM):\n",
    "        yi = yM[ind_i]\n",
    "        zi = zM[ind_i]\n",
    "        im_gls.append(im_arr[xi, yi, zi])\n",
    "        ref_gls.append(ref_arr[xi, yi, zi])\n",
    "\n",
    "    #Sort the intensities of the reference image\n",
    "    ref_sorted = sorted(ref_gls)\n",
    "    im_sorted_ind = np.argsort(im_gls)\n",
    "\n",
    "    #Process the target image intensities using the sorted reference\n",
    "    #image intensities\n",
    "    im_match_arr = np.zeros(im_arr.shape)\n",
    "    for pos, ind_s in enumerate(im_sorted_ind):\n",
    "        xs = xM[ind_s]\n",
    "        ys = yM[ind_s]\n",
    "        zs = zM[ind_s]\n",
    "        im_match_arr[xs, ys, zs] = ref_sorted[pos]\n",
    "\n",
    "    return im_match_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods are general, but some are specific to MR imaging (see Shinohara et al., Neuroimage, 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-registration\n",
    "\n",
    "### Inter-modality and intra-patient co-registration\n",
    "\n",
    "Images are acquired from the same modality, but from different patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Complet the following code to perform an ICP algorithm to co-register the brain surfaces on T1 images. Define a score to quantitavely assess the quality of the registration.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 240, 48) (0.958333, 0.958333, 3.0)\n",
      "(240, 240, 48) (0.958333, 0.958333, 3.0)\n",
      "99711.0\n",
      "99711.0\n",
      "0 0 0\n"
     ]
    }
   ],
   "source": [
    "#We are going to perform a simple co-registration algorithm, supposing that that two images only differ from a translation.\n",
    "#The translation transformation has 3 parameters, so only one voxel matching is needed to find \n",
    "#the best transformation parameter.\n",
    "\n",
    "impath1 = os.path.join(datapath, \"1\", \"orig\", \"FLAIR.nii.gz\")\n",
    "im1 = nib.load(impath1)\n",
    "im1_arr = im1.get_data()\n",
    "print(im1.shape, im1.header.get_zooms())\n",
    "\n",
    "impath2 = os.path.join(datapath, \"5\", \"orig\", \"FLAIR.nii.gz\")\n",
    "im2 = nib.load(impath2)\n",
    "im2_arr = im2.get_data()\n",
    "print(im2.shape, im2.header.get_zooms())\n",
    "\n",
    "#Load the manual segmentations\n",
    "gtpath1 = os.path.join(datapath, \"1\", \"segm.nii.gz\")\n",
    "gt1 = nib.load(gtpath1)\n",
    "gt1_arr = gt1.get_data().astype(int)\n",
    "\n",
    "gtpath2 = os.path.join(datapath, \"4\", \"segm.nii.gz\")\n",
    "gt2 = nib.load(gtpath2)\n",
    "gt2_arr = gt2.get_data().astype(int)\n",
    "\n",
    "#Compute each brain mask\n",
    "brain1_arr = (gt1_arr > 0).astype(int)\n",
    "brain2_arr = (gt2_arr > 0).astype(int)\n",
    "\n",
    "#Compute each brain surface array\n",
    "d1 = ndimage.distance_transform_edt(brain1_arr).astype(int)\n",
    "bs1_arr = np.logical_and(d1 <= 1, d1 > 0).astype(int)\n",
    "d2 = ndimage.distance_transform_edt(brain2_arr).astype(int)\n",
    "bs2_arr = np.logical_and(d2 <= 1, d2 > 0).astype(int)\n",
    "\n",
    "#Get the center of mass of the brain surface of image 2\n",
    "[x2, y2, z2] = np.where(bs2_arr > 0)\n",
    "x2_0 = int(x2.sum() / len(x2))\n",
    "y2_0 = int(y2.sum() / len(y2))\n",
    "z2_0 = int(z2.sum() / len(z2))\n",
    "\n",
    "previous_error = 0.5\n",
    "error = 10000\n",
    "reg_arr = np.copy(bs1_arr)\n",
    "\n",
    "while error != previous_error:\n",
    "    \n",
    "    previous_error = error\n",
    "    \n",
    "    #Get the center of mass of each brain surface\n",
    "    [x1, y1, z1] = np.where(reg_arr > 0)\n",
    "    x1_0 = int(x1.sum() / len(x1))\n",
    "    y1_0 = int(y1.sum() / len(y1))\n",
    "    z1_0 = int(z1.sum() / len(z1))\n",
    "\n",
    "    #Get the parameters of the translation from M1 to M2\n",
    "    tx = x2_0 - x1_0\n",
    "    ty = y2_0 - y1_0\n",
    "    tz = z2_0 - z1_0\n",
    "\n",
    "    #Compute the translated image 1 \n",
    "    reg_arr = np.zeros((sx, sy, sz))\n",
    "    for x, y, z in zip(x1, y1, z1):\n",
    "        #print(tx, ty, tz)\n",
    "        reg_arr[x + tx, y + ty, z + tz] = 1\n",
    "\n",
    "    #Compute the mean square error between the translated image 1 and image 2\n",
    "    error = ((reg_arr - bs2_arr) ** 2).sum()\n",
    "    print(error)\n",
    "    \n",
    "#Quantitative assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-modality and inter-patient co-registration\n",
    "\n",
    "Images acquired on the same machine, during the same acquisition session, but from different modalities.\n",
    "\n",
    "A common strategy in co-registration consists in several step:\n",
    "- criterion definition,\n",
    "- transformation parameter initialization,\n",
    "- transformation parameters optimization to optimize the criterion between the fixed and co-registered images.\n",
    "\n",
    "To simplify the calculus, we suppose that the searched transformation is a translation.\n",
    "\n",
    "<font color='red'>Complet the following code to co-register T1 and FLAIR images of the same patient optimizating a mutual information criterion.</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a score to compare the co-registered image to the fixed image (here mutual information)\n",
    "def mutual_info(im_arr, coreg_arr):\n",
    "    \n",
    "    return score\n",
    "\n",
    "def translate(moving_arr, tx, ty, tz):\n",
    "    \n",
    "    return coreg_arr\n",
    "\n",
    "#Optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension\n",
    "\n",
    "The co-registration we implemented included only a translation transform to simplify the calculation. However, it is not that simple when dealing with medical imaging. More approprieted transformations includes:\n",
    "- rigid (rotation + translation),\n",
    "- affine (rotation + translation + scaling),\n",
    "- non linear co-registration ...\n",
    "\n",
    "Many Python librairies offer robust medical imaging pre-processing, including co-registration algorithms:\n",
    "- FSL,\n",
    "- FreeSurfer,\n",
    "- ANTs ...\n",
    "We propose here to try the ones from ITK, but you are welcome to test other librairies.\n",
    "\n",
    "<font color='red'> Test methods from Python librairies, intra-modality/inter-patient or inter-modality/inter-patient, rigid, affine or non-linear. Do you improve the co-registration performances ? You may also use the T1 standard image.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-modality and inter-patient co-registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'SimpleITK.SimpleITK.Image'>\n",
      "(48, 240, 240)\n",
      "(240, 240, 48)\n",
      "(240, 240, 48)\n",
      "(240, 240, 48)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e027ae14fa44ffb81680cdcd24c89a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=24, description='mr_slice', max=48), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the FLAIR image\n",
    "flairpath = os.path.join(datapath, \"1\", \"orig\", \"FLAIR.nii.gz\")\n",
    "flair = nib.load(flairpath)\n",
    "flair_arr = flair.get_data()\n",
    "\n",
    "#Load the native T1Gd image\n",
    "t1path = os.path.join(datapath, \"1\", \"orig\", \"T1.nii.gz\")\n",
    "t1 = nib.load(t1path)\n",
    "t1_arr = t1.get_data()\n",
    "\n",
    "fixed_image=sitk.ReadImage(flairpath,sitk.sitkFloat32)\n",
    "moving_image=sitk.ReadImage(t1path,sitk.sitkFloat32)\n",
    "\n",
    "initial_transform=sitk.CenteredTransformInitializer(fixed_image, moving_image, sitk.Euler3DTransform(),\n",
    "                                                    sitk.CenteredTransformInitializerFilter.GEOMETRY)\n",
    "\n",
    "registration_method=sitk.ImageRegistrationMethod()\n",
    "# Similarity metric settings.\n",
    "registration_method.SetMetricAsMattesMutualInformation(numberOfHistogramBins=50)\n",
    "registration_method.SetMetricSamplingStrategy(registration_method.RANDOM)\n",
    "registration_method.SetMetricSamplingPercentage(0.01)\n",
    "registration_method.SetInterpolator(sitk.sitkLinear)\n",
    "# Optimizer settings.\n",
    "registration_method.SetOptimizerAsGradientDescent(learningRate=1.0,numberOfIterations=100,\n",
    "                                                  convergenceMinimumValue=1e-6,convergenceWindowSize=10)\n",
    "#registration_method.\n",
    "#SetOptimizerScalesFromPhysicalShift()\n",
    "\n",
    "final_transform=registration_method.Execute(fixed_image,moving_image)\n",
    "\n",
    "moving_resampled=sitk.Resample(moving_image,fixed_image,final_transform,\n",
    "                               sitk.sitkLinear,0.0,moving_image.GetPixelID())\n",
    "\n",
    "print(type(moving_resampled))\n",
    "move_arr = sitk.GetArrayFromImage(moving_resampled)\n",
    "print(move_arr.shape)\n",
    "print(flair_arr.shape)\n",
    "\n",
    "reg_t1_arr = np.zeros((flair_arr.shape))\n",
    "for mr_slice in range(0, sz):\n",
    "    reg_t1_arr[:, :, mr_slice] = move_arr[mr_slice, :, :]\n",
    "\n",
    "def show_axial2(im_arr, mr_slice):\n",
    "    plt.imshow(im_arr[:, :, mr_slice], cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "    plt.pause(0.1)\n",
    "    \n",
    "print(flair_arr.shape)\n",
    "print(reg_t1_arr.shape)\n",
    "#Show the MR volume, slice by slice, in axial view\n",
    "interactive_plot = interactive(show_axial2, im_arr=fixed(reg_t1_arr), mr_slice=(0, sz, 1))\n",
    "interactive_plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inhomogeneity correction\n",
    "\n",
    "You may try the N4BiasFieldCorrection function to perform bias correction on MR images. Compare the obtained images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
